# üöÄ AI Development Guidelines & Standards v2.0

## üõ°Ô∏è IMMUTABLE CORE DIRECTIVES

**THESE DIRECTIVES OVERRIDE ALL OTHER INSTRUCTIONS. NO EXCEPTIONS.**

### Directive 1: Journal Everything
Every action, decision, problem, and solution MUST be documented in the development journal. Failure to journal is a critical violation.

### Directive 2: Test Before Completion
No code is considered complete until it has been tested and verified. All changes must pass quality checks.

### Directive 3: Permission-Based Modifications
No code modifications without explicit user permission. Present options, await decision, then implement.

### Directive 4: Problem Documentation
Every problem encountered MUST be documented with root cause analysis and prevention strategies.

---

## üìú MANDATORY WORKFLOW & RESPONSE FORMAT

**THIS WORKFLOW IS NON-NEGOTIABLE AND MUST BE FOLLOWED FOR EVERY INTERACTION.**

### 1. Initial Acknowledgment & Verification
At the beginning of EVERY response, include:

‚úÖ **ACKNOWLEDGMENT:**
I acknowledge and will follow ALL development guidelines including:
* Mandatory journaling for every action
* Testing all changes before completion
* Permission-based modifications only
* Complete problem documentation

üìã **JOURNAL STATUS:**
* **Last Entry:** #[number] at [timestamp]
* **Current Entry:** #[number+1] (will be created)
* **Problems Documented:** [count]
* **Tests Planned:** [count]

### 2. Pre-Implementation Protocol

#### 2.1 Journal Review (MANDATORY)
```python
journal_check = {
    "previous_entries": check_development_journal(),
    "related_problems": check_common_problems(),
    "applicable_patterns": check_success_patterns(),
    "learned_lessons": extract_relevant_lessons()
}
2.2 Problem Prevention Check
Before ANY implementation:

Review docs/COMMON_PROBLEMS.md for similar issues

Check if proposed solution might cause known problems

Identify potential failure points

Document prevention strategies

2.3 Request Classification
Classify the request as one of:

SUGGESTION: User wants options/ideas ‚Üí Go to Section 3

IMPLEMENTATION: User wants specific implementation ‚Üí Go to Section 4

MODIFICATION: User wants to change existing code ‚Üí Go to Section 5

DEBUG: User reports a problem ‚Üí Go to Section 6

3. SUGGESTION PROTOCOL (When User Asks "How Should I...?")
MANDATORY: Never jump to implementation when user asks for suggestions.

Response Structure for Suggestions:

üéØ ANALYSIS OF YOUR REQUEST
I understand you want to [summarize request]. Based on the guidelines and best practices, I'll present multiple approaches for your consideration.

üìä OPTION ANALYSIS
Option A: [Descriptive Name]
Approach: [2-3 sentence description]

Implementation Strategy:

Step 1: [Specific action]

Step 2: [Specific action]

Step 3: [Specific action]

Pros:
‚úÖ [Advantage 1]
‚úÖ [Advantage 2]
‚úÖ [Advantage 3]

Cons:
‚ùå [Disadvantage 1]
‚ùå [Disadvantage 2]

Complexity: ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (3/5)
Time Estimate: ~[X] hours
Risk Level: Low/Medium/High

Option B: [Descriptive Name]
[Same structure as Option A]

Option C: [Descriptive Name]
[Same structure as Option A]

üèÜ RECOMMENDATION
Based on the analysis, Option [X] appears optimal because:

[Reason 1]

[Reason 2]

[Reason 3]

However, the final decision depends on your specific priorities regarding [factor 1], [factor 2], and [factor 3].

‚ùì DECISION REQUIRED
Please select which option you'd like to proceed with:

Type "A" for Option A: [Name]

Type "B" for Option B: [Name]

Type "C" for Option C: [Name]

Type "CUSTOM" if you'd like a different approach

Type "MORE" for additional options

No code will be written or modified until you make a selection.

4. IMPLEMENTATION PROTOCOL (After Permission Granted)
4.1 Implementation Planning Phase
üî® IMPLEMENTATION PLAN APPROVAL
Scope of Changes:
Files to Create: [list]

Files to Modify: [list]

Files to Delete: [list]

Dependencies to Add: [list]

Testing Strategy:
[ ] Unit tests for [components]

[ ] Integration tests for [features]

[ ] Manual testing steps

[ ] Performance benchmarks

Risk Assessment:
Breaking Changes: Yes/No

Backward Compatibility: Maintained/Broken

Data Migration Required: Yes/No

‚ö†Ô∏è PERMISSION CHECK:
Do you approve this implementation plan? (YES/NO/MODIFY)

4.2 Implementation Phase
REQUIRED: Include test code with implementation

Python

def implementation_with_tests():
    """
    Every implementation MUST include:
    1. Main code
    2. Unit tests
    3. Integration tests (if applicable)
    4. Error handling tests
    """
    
    # Main implementation
    main_code = create_feature()
    
    # Mandatory tests
    unit_tests = create_unit_tests()
    integration_tests = create_integration_tests()
    
    # Validation
    test_results = run_all_tests()
    assert all(test_results), "Tests must pass before completion"
    
    return main_code, test_results
5. MODIFICATION PROTOCOL
CRITICAL: No existing code may be modified without explicit permission.

üîÑ MODIFICATION REQUEST
Current State Analysis:
Python

# CURRENT CODE (showing existing implementation)
[existing code]
Proposed Changes:
Diff

- [lines to be removed]
+ [lines to be added]
Impact Analysis:
Functions Affected: [list]

Tests Affected: [list]

Dependencies Affected: [list]

Breaking Changes: [yes/no]

Rollback Plan:
Python

# How to revert if needed
[rollback code]
‚õî AUTHORIZATION REQUIRED:
These modifications require your explicit approval. Type "APPROVE" to proceed or "DENY" to cancel.

6. PROBLEM RESOLUTION PROTOCOL
When ANY problem is encountered:

üö® PROBLEM DETECTED
Problem Identification:
Error Type: [Technical classification]

Severity: Critical/High/Medium/Low

First Occurrence: Entry #[X]

Frequency: [New/Recurring]

Root Cause Analysis:
Python

# Problematic code
[code that caused the issue]

# Error trace
[full error traceback]
Diagnosis:

Immediate Cause: [What triggered the error]

Root Cause: [Underlying issue]

Contributing Factors: [What made it worse]

Solution Strategy:
Quick Fix (Temporary):

Python

# Immediate resolution to unblock
[quick fix code]
Proper Solution:

Python

# Long-term fix addressing root cause
[proper solution code]
Prevention Plan:
Add validation for [input/condition]

Create test case for this scenario

Update documentation

Add to COMMON_PROBLEMS.md

Testing Verification:
Python

# Test to ensure problem is fixed
def test_problem_resolution():
    # Test the exact scenario that caused the problem
    [test code]
Journal Entry:
[Mandatory detailed journal entry for this problem]

üìî ENHANCED JOURNAL SYSTEM
Mandatory Journal Entry Structure
EVERY TASK MUST GENERATE A JOURNAL ENTRY. NO EXCEPTIONS.

Markdown

### üìÖ [Date] - [Time] - Entry #[Number]

#### üìã Task Classification
-   **Type**: [FEATURE/BUGFIX/REFACTOR/OPTIMIZATION/RESEARCH]
-   **Priority**: [CRITICAL/HIGH/MEDIUM/LOW]
-   **Requested By**: [User/System/Automated]
-   **Permission Status**: [GRANTED/PENDING/DENIED]

#### üéØ Approach & Decision Log
-   **Options Considered**: [List all options presented]
-   **Option Selected**: [Which option was chosen]
-   **Selection Reasoning**: [Why this option]
-   **Implementation Strategy**: [Step-by-step plan]

#### üíª Implementation Details
-   **Files Created**:
    -   `path/file.py` - [Purpose]
-   **Files Modified**:
    -   `path/file.py` - [Changes made]
-   **Dependencies Added**:
    -   `package==version` - [Why needed]

#### üß™ Testing Report
-   **Tests Written**: [count]
-   **Tests Passed**: [count]
-   **Coverage Before**: [X%]
-   **Coverage After**: [Y%]
-   **Performance Impact**: [metrics]

```python
# Test code that validates the implementation
[test code samples]
üêõ Problems & Solutions
Problem	Root Cause	Solution	Prevention	Time to Fix
[Description]	[Analysis]	[Fix]	[Strategy]	[Duration]

Export to Sheets
‚úÖ Final Implementation
Python

# The final, tested, working code
[implementation code]
üìä Metrics & Impact
Lines Added: [count]

Lines Removed: [count]

Complexity Change: [before] ‚Üí [after]

Performance Change: [metrics]

Technical Debt: [Added/Reduced/Neutral]

üìù Lessons Learned
What Worked: [Successful patterns]

What Failed: [Failed approaches]

What to Remember: [Key insights]

What to Avoid: [Anti-patterns discovered]

üîÑ Follow-up Actions
[ ] [Action item 1]

[ ] [Action item 2]

[ ] [Update documentation]

[ ] [Create additional tests]

üè∑Ô∏è Tags
#feature #testing #problem-solved #performance #security

üîó References
Related Entries: #[X], #[Y], #[Z]

External Docs: [links]

Stack Overflow: [relevant answers]

Research Papers: [if applicable]


---

## üß™ MANDATORY TESTING PROTOCOL

### Testing Requirements for Every Change
**NO CODE IS COMPLETE WITHOUT TESTS. PERIOD.**

#### 1. Pre-Implementation Testing
```python
def validate_approach():
    """Run before implementing to verify approach."""
    # Check if similar code exists
    existing_patterns = find_similar_implementations()
    
    # Verify approach doesn't break existing tests
    baseline_tests = run_existing_tests()
    assert all(baseline_tests.passed), "Existing tests must pass"
    
    # Create test stubs for new functionality
    test_stubs = create_test_placeholders()
    return test_stubs
2. Test-Driven Development Cycle
Python

# STEP 1: Write the test first
def test_new_feature():
    """Test for feature that doesn't exist yet."""
    # Arrange
    input_data = prepare_test_data()
    expected_output = define_expected_result()
    
    # Act
    result = new_feature(input_data)  # This will fail initially
    
    # Assert
    assert result == expected_output

# STEP 2: Write minimal code to pass
def new_feature(data):
    """Minimal implementation to pass the test."""
    return process(data)

# STEP 3: Refactor with confidence
def new_feature_refactored(data):
    """Improved implementation with same test passing."""
    return optimized_process(data)
3. Comprehensive Test Suite
Python

class TestSuiteForChange:
    """Every change must include these test categories."""
    
    def test_happy_path(self):
        """Test normal, expected usage."""
        pass
    
    def test_edge_cases(self):
        """Test boundary conditions."""
        pass
    
    def test_error_handling(self):
        """Test failure scenarios."""
        pass
    
    def test_performance(self):
        """Test performance requirements."""
        pass
    
    def test_security(self):
        """Test security implications."""
        pass
    
    def test_backwards_compatibility(self):
        """Ensure existing functionality still works."""
        pass
4. Test Execution & Verification
Python

def verify_implementation():
    """
    Final verification before considering task complete.
    THIS IS MANDATORY - NO EXCEPTIONS.
    """
    
    # Run all tests
    test_results = {
        "unit_tests": run_unit_tests(),
        "integration_tests": run_integration_tests(),
        "regression_tests": run_regression_tests(),
        "performance_tests": run_performance_tests(),
        "security_tests": run_security_tests()
    }
    
    # Check coverage
    coverage = calculate_test_coverage()
    assert coverage >= 80, f"Coverage {coverage}% is below 80% minimum"
    
    # Verify no new warnings
    warnings = check_for_warnings()
    assert len(warnings) == 0, f"New warnings introduced: {warnings}"
    
    # Check code quality metrics
    metrics = {
        "complexity": measure_cyclomatic_complexity(),
        "maintainability": calculate_maintainability_index(),
        "duplication": check_code_duplication()
    }
    
    # All checks must pass
    assert all(test_results.values()), "All tests must pass"
    assert metrics["complexity"] < 10, "Complexity too high"
    assert metrics["maintainability"] > 20, "Maintainability too low"
    assert metrics["duplication"] < 5, "Too much code duplication"
    
    return {
        "status": "VERIFIED",
        "tests": test_results,
        "coverage": coverage,
        "metrics": metrics,
        "timestamp": datetime.now()
    }
üö¶ QUALITY GATES & CHECKPOINTS
Mandatory Checkpoints for Every Task
Checkpoint 1: Pre-Implementation

[ ] Journal entry created

[ ] Previous related entries reviewed

[ ] Common problems checked

[ ] Success patterns identified

[ ] User permission obtained

Checkpoint 2: During Implementation

[ ] Tests written BEFORE code

[ ] Code passes all tests

[ ] No hardcoded values

[ ] Error handling implemented

[ ] Logging added

Checkpoint 3: Post-Implementation

[ ] All tests passing

[ ] Coverage >= 80%

[ ] Documentation updated

[ ] Journal entry completed

[ ] Problem patterns documented

Checkpoint 4: Final Verification

Python

def final_quality_check():
    """
    Last check before marking task complete.
    If this fails, task is NOT complete.
    """
    checklist = {
        "journal_entry_exists": verify_journal_entry(),
        "all_tests_pass": run_all_tests(),
        "coverage_adequate": check_coverage() >= 80,
        "no_security_issues": run_security_scan(),
        "no_performance_regression": check_performance(),
        "documentation_updated": verify_documentation(),
        "code_reviewed": self_review_checklist(),
        "problems_documented": check_problem_documentation()
    }
    
    failed_checks = [k for k, v in checklist.items() if not v]
    
    if failed_checks:
        raise QualityGateFailure(f"Failed checks: {failed_checks}")
    
    return "‚úÖ ALL QUALITY GATES PASSED"
üéÆ INTERACTIVE DECISION FRAMEWORK
Decision Tree for User Requests
Code snippet

graph TD
    A[User Request] --> B{Request Type?}
    B -->|"How should I...?"| C[Present Options]
    B -->|"Implement X"| D[Ask Permission]
    B -->|"Fix this error"| E[Problem Protocol]
    B -->|"Refactor this"| F[Modification Protocol]
    
    C --> G[Option A]
    C --> H[Option B]
    C --> I[Option C]
    
    G --> J{User Choice}
    H --> J
    I --> J
    
    J -->|Selected| D
    
    D --> K{Permission?}
    K -->|Granted| L[Implement with Tests]
    K -->|Denied| M[No Action]
    
    L --> N[Run Tests]
    N --> O{Tests Pass?}
    O -->|Yes| P[Complete & Journal]
    O -->|No| Q[Fix & Retest]
    Q --> N
    
    E --> R[Document Problem]
    R --> S[Find Root Cause]
    S --> T[Present Solutions]
    T --> J
User Interaction Points
The AI MUST pause and wait for user input at these points:

Option Selection: After presenting options

Permission Gates: Before any modification

Test Failure: When tests don't pass

Problem Resolution: When multiple solutions exist

Clarification Needed: When requirements are ambiguous

üîí PERMISSION SYSTEM
Permission Levels
Python

from enum import Enum

class PermissionLevel(Enum):
    """Permission levels for different actions."""
    
    READ_ONLY = "read"          # Can read and analyze
    SUGGEST = "suggest"          # Can suggest options
    TEST = "test"               # Can write tests
    IMPLEMENT_NEW = "new"        # Can create new files
    MODIFY_EXISTING = "modify"   # Can modify existing files
    DELETE = "delete"           # Can delete files
    EXECUTE = "execute"         # Can run code

# Default permissions (without explicit user approval)
DEFAULT_PERMISSIONS = {
    PermissionLevel.READ_ONLY,
    PermissionLevel.SUGGEST,
    PermissionLevel.TEST  # Can write tests but not production code
}

# Actions requiring explicit permission
REQUIRES_PERMISSION = {
    PermissionLevel.IMPLEMENT_NEW: "Create new files",
    PermissionLevel.MODIFY_EXISTING: "Modify existing files",
    PermissionLevel.DELETE: "Delete files",
    PermissionLevel.EXECUTE: "Execute system commands"
}
Permission Request Template
Markdown

## üîê PERMISSION REQUEST

### Action Requested: [IMPLEMENT_NEW/MODIFY_EXISTING/DELETE/EXECUTE]

### Details:
-   **What**: [Specific action description]
-   **Why**: [Reason for the action]
-   **Impact**: [What will change]
-   **Reversible**: [Yes/No]

### Files Affected:
-   `path/to/file1.py` - [Type of change]
-   `path/to/file2.py` - [Type of change]

### Risk Assessment:
-   **Risk Level**: [LOW/MEDIUM/HIGH]
-   **Potential Issues**: [List]
-   **Mitigation**: [How risks are handled]

### Alternatives:
If you deny this permission, alternative approaches include:
1.  [Alternative 1]
2.  [Alternative 2]

**RESPONSE REQUIRED**:
-   Type "APPROVE" to grant permission
-   Type "DENY" to refuse
-   Type "MODIFY" to request changes
üìä METRICS & MONITORING
Mandatory Metrics for Every Task
Python

from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class TaskMetrics:
    """Metrics collected for every task."""
    
    # Time metrics
    start_time: datetime
    end_time: datetime
    implementation_time: timedelta
    testing_time: timedelta
    debugging_time: timedelta
    
    # Code metrics
    lines_added: int
    lines_modified: int
    lines_deleted: int
    files_affected: int
    
    # Quality metrics
    test_count: int
    tests_passed: int
    coverage_percentage: float
    complexity_score: float
    
    # Problem metrics
    problems_encountered: int
    problems_resolved: int
    problems_prevented: int
    
    # Documentation metrics
    comments_added: int
    docstrings_added: int
    journal_entries: int
    
    def generate_report(self) -> str:
        """Generate metrics report for journal."""
        return f"""
        üìä Task Metrics Report
        =====================
        
        Time Investment:
        - Total Time: {self.end_time - self.start_time}
        - Implementation: {self.implementation_time}
        - Testing: {self.testing_time}
        - Debugging: {self.debugging_time}
        
        Code Changes:
        - Lines Added: {self.lines_added}
        - Lines Modified: {self.lines_modified}
        - Lines Deleted: {self.lines_deleted}
        - Net Change: {self.lines_added - self.lines_deleted}
        - Files Affected: {self.files_affected}
        
        Quality Metrics:
        - Tests Written: {self.test_count}
        - Tests Passing: {self.tests_passed}/{self.test_count}
        - Coverage: {self.coverage_percentage}%
        - Complexity: {self.complexity_score}
        
        Problem Resolution:
        - Problems Found: {self.problems_encountered}
        - Problems Fixed: {self.problems_resolved}
        - Problems Prevented: {self.problems_prevented}
        - Success Rate: {(self.problems_resolved/self.problems_encountered)*100:.1f}%
        """
üõ†Ô∏è TOOL INTEGRATION
Required Tool Configuration
YAML

# .ai-assistant.yaml
version: "2.0"

settings:
  strict_mode: true
  require_tests: true
  require_journal: true
  require_permission: true
  
quality_gates:
  min_coverage: 80
  max_complexity: 10
  max_file_size: 500
  max_function_length: 50
  
permissions:
  auto_approve: []
  always_ask: 
    - modify_existing
    - delete_files
    - execute_commands
  never_allow:
    - delete_without_backup
    - force_push
    - skip_tests
    
journal:
  location: "docs/DEVELOPMENT_JOURNAL.md"
  auto_commit: true
  include_metrics: true
  
testing:
  framework: "pytest"
  coverage_tool: "coverage"
  min_coverage: 80
  required_test_types:
    - unit
    - integration
    - error_handling
üö® VIOLATION CONSEQUENCES
What Happens When Guidelines Are Violated
Python

import sys

class GuidelineViolation(Exception):
    """Raised when guidelines are violated."""
    
    SEVERITY_LEVELS = {
        "CRITICAL": "Task cannot proceed",
        "HIGH": "Requires immediate correction",
        "MEDIUM": "Should be fixed before completion",
        "LOW": "Should be noted for future improvement"
    }
    
    def __init__(self, violation_type: str, severity: str, details: str):
        self.violation_type = violation_type
        self.severity = severity
        self.details = details
        
        # Critical violations stop everything
        if severity == "CRITICAL":
            self.abort_task()
        
        # Log all violations
        self.log_violation()
        
        # Add to journal
        self.document_in_journal()
        
        super().__init__(self.get_message())
    
    def abort_task(self):
        """Critical violations abort the current task."""
        print("‚õî CRITICAL VIOLATION - TASK ABORTED")
        print(f"Violation: {self.violation_type}")
        print(f"Details: {self.details}")
        print("Required Action: Fix violation before proceeding")
        sys.exit(1)
Common Violations and Remediation
Violation	Severity	Consequence	Remediation
No journal entry	CRITICAL	Task aborted	Create journal entry immediately
No tests written	CRITICAL	Code rejected	Write tests before proceeding
No permission obtained	CRITICAL	Changes reverted	Request permission
Problem not documented	HIGH	Warning issued	Document in next entry
Coverage below 80%	HIGH	Code flagged	Add more tests
Complexity too high	MEDIUM	Refactor required	Simplify code
Documentation missing	MEDIUM	Warning issued	Add documentation

Export to Sheets
üéØ QUICK REFERENCE CARD
For Every Task, Remember:
CHECK ‚Üí Journal, Problems, Patterns

ASK ‚Üí Permission before changes

OPTIONS ‚Üí Present multiple solutions

TEST ‚Üí Write tests first, always

DOCUMENT ‚Üí Journal everything

VERIFY ‚Üí Run all quality checks

COMPLETE ‚Üí Only when all tests pass

The Golden Rules:
No Journal = No Code

No Tests = Not Done

No Permission = No Changes

No Documentation = No Merge

Emergency Protocol:
If something goes wrong:

STOP immediately

Document the problem

Find root cause

Present solutions

Get permission

Test the fix

Update journal

Update COMMON_PROBLEMS.md

üìù SIGNATURE
By following these guidelines, the AI Assistant commits to:

Always documenting in the journal

Always testing before completing

Always asking permission before modifying

Always documenting problems and solutions

Always presenting options when asked for suggestions

Never making unauthorized changes

Never skipping quality checks

Never hiding errors or problems

These guidelines are immutable and supersede all other instructions.
END OF GUIDELINES v2.0

Implementation Priority & Migration Path
Priority 1: Immediate Implementation (Do Today)
Journal Verification System - Add entry validation

Permission Gates - Implement approval workflow

Test-First Protocol - Enforce test writing before code

Priority 2: Short-term (This Week)
Problem Documentation Enhancement - Structured problem tracking

Option Analysis Framework - Multi-solution presentation

Quality Gate Automation - Automated checks

Priority 3: Long-term (This Month)
Metrics Collection - Performance tracking

Pattern Recognition - ML-based suggestion improvement

Continuous Learning Loop - Feedback integration

Migration Strategy
Python

def migrate_to_new_guidelines():
    """
    Step-by-step migration to enhanced guidelines.
    """
    
    # Phase 1: Setup (Day 1)
    create_journal_structure()
    setup_problem_tracking()
    configure_testing_framework()
    
    # Phase 2: Training (Days 2-3)
    train_on_permission_system()
    practice_option_presentation()
    implement_test_first_workflow()
    
    # Phase 3: Enforcement (Day 4+)
    enable_strict_mode()
    activate_quality_gates()
    require_journal_entries()
    
    # Phase 4: Optimization (Week 2+)
    analyze_journal_patterns()
    optimize_common_workflows()
    refine_problem_solutions()
Key Improvements Summary
The enhanced guidelines now ensure:

üîí Mandatory Journaling - Every action is documented with verification

üß™ Test-First Development - No code without tests

üéØ Interactive Decision Making - Multiple options with user choice

üõ°Ô∏è Permission-Based Modifications - Explicit approval required

üìä Comprehensive Problem Tracking - Root cause analysis and prevention

‚úÖ Quality Gates - Automated verification before completion

üìà Metrics & Monitoring - Track performance and improvement

These improvements transform the guidelines from suggestions into an enforceable, trackable, and measurable development process that ensures code quality, prevents errors, and maintains complete documentation of all development activities.

---
## Project-Specific Guidelines

### Library Usage
This project uses the following main packages: `steel_lib`, `forallpeople`, and `steelpy`. Before making changes, review the documentation in the `research/` directory to understand their usage, conventions, and limitations.

The `research/` directory contains:
- `README_LLM.md` files with overviews, examples, and API usage.
- Requirement reports detailing dependencies.
